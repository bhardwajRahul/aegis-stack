"""
AI service CLI commands.

Command-line interface for AI service management and chat functionality.
"""

import logging
from contextlib import contextmanager

import typer
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from rich.table import Table

from ..core.config import settings
from ..core.log import setup_logging
from ..services.ai.config import get_ai_config

# Initialize logging at module load
setup_logging()
from ..services.ai.models import (
    AIProvider,
    MessageRole,
    get_free_providers,
    get_provider_capabilities,
)

app = typer.Typer(help="AI service management and chat commands")
console = Console()


@contextmanager
def suppress_logs(level: int = logging.ERROR):
    """
    Context manager to temporarily suppress logs during interactive chat.

    Sets the root logger to ERROR level to hide INFO/DEBUG/WARNING logs while
    preserving ERROR logs for critical issues.

    Args:
        level: Minimum log level to show (default: ERROR)
    """
    # Get the root logger and remember original level
    root_logger = logging.getLogger()
    original_level = root_logger.level

    try:
        # Temporarily raise log level to suppress info/debug logs
        root_logger.setLevel(level)
        yield
    finally:
        # Restore original log level
        root_logger.setLevel(original_level)


@app.command()
def status() -> None:
    """Show AI service status, configuration, and validation."""
    ai_config = get_ai_config(settings)

    typer.secho("AI Service Status", fg=typer.colors.BLUE, bold=True)
    typer.secho("=" * 40, dim=True)

    # Basic info
    typer.echo(
        typer.style("Engine: ", fg=typer.colors.CYAN) + "{{ cookiecutter.ai_framework }}"
    )
    status_color = typer.colors.GREEN if ai_config.enabled else typer.colors.RED
    status_text = "Enabled" if ai_config.enabled else "Disabled"
    typer.echo(
        typer.style("Status: ", fg=typer.colors.CYAN)
        + typer.style(status_text, fg=status_color)
    )
    typer.echo(
        typer.style("Provider: ", fg=typer.colors.CYAN) + ai_config.provider.value
    )
    typer.echo(typer.style("Model: ", fg=typer.colors.CYAN) + str(ai_config.model))
    typer.echo(
        typer.style("Temperature: ", fg=typer.colors.CYAN)
        + str(ai_config.temperature)
    )
    typer.echo(
        typer.style("Max Tokens: ", fg=typer.colors.CYAN) + str(ai_config.max_tokens)
    )

    # API Key status
    provider_config = ai_config.get_provider_config(settings)
    api_key_color = typer.colors.GREEN if provider_config.api_key else typer.colors.RED
    api_key_text = "Set" if provider_config.api_key else "Not set"
    typer.echo(
        typer.style("API Key: ", fg=typer.colors.CYAN)
        + typer.style(api_key_text, fg=api_key_color)
    )

    # Validation
    typer.echo("")
    errors = ai_config.validate_configuration(settings)
    if not errors:
        typer.secho("✓ Configuration valid", fg=typer.colors.GREEN)
        capabilities = get_provider_capabilities(ai_config.provider)
        if capabilities.free_tier_available:
            typer.echo("  " + typer.style("Free tier", fg=typer.colors.CYAN))
        if capabilities.supports_streaming:
            typer.echo("  " + typer.style("Streaming supported", fg=typer.colors.CYAN))
    else:
        typer.secho("✗ Configuration issues:", fg=typer.colors.RED)
        for error in errors:
            typer.echo("  " + typer.style("•", fg=typer.colors.RED) + f" {error}")

        # Suggest free providers if API key issues
        if any("API key" in error for error in errors):
            free_providers = get_free_providers()
            if free_providers:
                providers_list = ", ".join(p.value for p in free_providers)
                typer.echo("")
                typer.echo(
                    typer.style("Tip: ", fg=typer.colors.YELLOW)
                    + f"Try free providers: {providers_list}"
                )

    # Available providers count
    available = ai_config.get_available_providers(settings)
    typer.echo("")
    typer.echo(
        typer.style("Available providers: ", fg=typer.colors.CYAN)
        + f"{len(available)} (run 'ai providers' to list)"
    )


@app.command()
def providers() -> None:
    """List all available AI providers."""
    ai_config = get_ai_config(settings)
    available = ai_config.get_available_providers(settings)
    free_providers = get_free_providers()

    table = Table(title="AI Providers", width=75)
    table.add_column("Provider", style="cyan", width=9)
    table.add_column("Status", style="green", width=26, no_wrap=True)
    table.add_column("Free", style="yellow", width=4)
    table.add_column("Features", style="blue", width=18)

    for provider in AIProvider:
        capabilities = get_provider_capabilities(provider)
        is_available = provider in available
        is_current = provider == ai_config.provider

        if is_available:
            status = "Available"
        else:
            # Make status more informative about what's missing
            if provider == AIProvider.PUBLIC:
                status = "[FAIL] Error"  # Shouldn't happen for PUBLIC
            else:
                # Show abbreviated environment variable name
                env_var = f"{provider.value.upper()}_API_KEY"
                status = f"[red]Need {env_var}[/red]"

        if is_current:
            status += " (current)"

        free_tier = "Yes" if provider in free_providers else "No"

        features = []
        if capabilities.supports_streaming:
            features.append("Stream")
        if capabilities.supports_function_calling:
            features.append("Functions")
        if capabilities.supports_vision:
            features.append("Vision")

        table.add_row(
            provider.value,
            status,
            free_tier,
            ", ".join(features) if features else "Basic",
        )

    console.print(table)


@app.command()
def chat(
    message: str | None = typer.Argument(None, help="Message to send to AI"),
    stream: bool = typer.Option(
        True, "--stream/--no-stream", help="Enable streaming output"
    ),
    conversation_id: str | None = typer.Option(
        None, "--conversation-id", "-c", help="Continue existing conversation"
    ),
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show conversation metadata"
    ),
) -> None:
    """Send a chat message or start interactive session.

    Examples:
        ai chat "What is Python?"     - Send a single message
        ai chat                       - Start interactive session
        ai chat -c abc123 "Continue"  - Continue a conversation
    """
    import asyncio

    from app.services.ai.service import AIService

    async def run_chat() -> None:
        try:
            with suppress_logs():
                ai_service = AIService(settings)

                if message:
                    # Single message mode
                    await _send_message(
                        ai_service, message, conversation_id, user_id, stream, verbose
                    )
                else:
                    # Interactive session mode
                    await _interactive_chat_session(ai_service, conversation_id)
        except KeyboardInterrupt:
            typer.echo("\nChat interrupted", err=True)
            raise typer.Exit(1)
        except Exception as e:
            typer.echo(f"Error: {e}", err=True)
            raise typer.Exit(1)

    asyncio.run(run_chat())


@app.command()
def conversations(
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
    limit: int = typer.Option(
        10, "--limit", "-l", help="Number of conversations to show"
    ),
) -> None:
    """List conversations for a user."""
    from app.services.ai.service import AIService

    with suppress_logs():
        ai_service = AIService(settings)
    convos = ai_service.list_conversations(user_id)[:limit]

    if not convos:
        typer.echo(f"No conversations found for user: {user_id}")
        return

    typer.echo(f"Conversations for {user_id}:")
    typer.echo("")

    for conv in convos:
        title = conv.title or "Untitled"
        messages = conv.get_message_count()
        updated = conv.updated_at.strftime("%Y-%m-%d %H:%M")

        typer.echo(f"• {conv.id[:8]}... - {title}")
        typer.echo(f"  {messages} messages | {updated}")
        typer.echo("")


@app.command()
def history(
    conversation_id: str = typer.Argument(..., help="Conversation ID"),
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
) -> None:
    """View conversation history."""
    from app.services.ai.service import AIService

    with suppress_logs():
        ai_service = AIService(settings)
    conversation = ai_service.get_conversation(conversation_id)

    if not conversation:
        typer.echo(f"Error: Conversation not found: {conversation_id}")
        raise typer.Exit(1)

    # Check if user owns conversation
    if conversation.metadata.get("user_id") != user_id:
        typer.echo("Error: Access denied: You don't own this conversation")
        raise typer.Exit(1)

    typer.echo(f"Conversation: {conversation_id}")
    if conversation.title:
        typer.echo(f"Title: {conversation.title}")
    typer.echo(f"Provider: {conversation.provider.value}")
    typer.echo(f"Messages: {conversation.get_message_count()}")
    typer.echo("")

    for i, msg in enumerate(conversation.messages):
        timestamp = msg.timestamp.strftime("%H:%M:%S")
        role_icon = "" if msg.role == MessageRole.USER else ""

        typer.echo(f"{role_icon} [{timestamp}] {msg.content}")
        if i < len(conversation.messages) - 1:
            typer.echo("")


{% if cookiecutter.ai_backend != "memory" %}
# ============================================================================
# Usage Statistics Command (requires database backend)
# ============================================================================


@app.command()
def usage(
    url: str = typer.Option(
        None,
        "--url",
        "-u",
        help="API base URL (default: localhost:8000)",
    ),
    user_id: str | None = typer.Option(
        None,
        "--user-id",
        help="Filter by user ID",
    ),
    recent: int = typer.Option(
        10,
        "--recent",
        "-r",
        help="Number of recent activities to show",
        min=1,
        max=50,
    ),
    json_output: bool = typer.Option(
        False,
        "--json",
        "-j",
        help="Output raw JSON",
    ),
) -> None:
    """
    Show AI usage statistics.

    Displays comprehensive LLM usage analytics including token counts,
    costs, model breakdown, and recent activity.

    Examples:
        ai usage              - Show full usage report
        ai usage --json       - Output raw JSON data
        ai usage -r 20        - Show 20 recent activities
    """
    import asyncio
    import json
    import sys
    from datetime import datetime

    import httpx
    from pydantic import BaseModel

    from app.core.constants import APIEndpoints, Defaults
    from app.core.formatting import format_cost, format_number, format_percentage

    # Response models matching API schema
    class ModelUsageStats(BaseModel):
        model_id: str
        model_title: str
        vendor: str
        vendor_color: str
        requests: int
        tokens: int
        cost: float
        percentage: float

    class RecentActivity(BaseModel):
        timestamp: str
        model: str
        tokens: int
        cost: float
        success: bool
        action: str

    class UsageStatsResponse(BaseModel):
        total_tokens: int
        input_tokens: int
        output_tokens: int
        total_cost: float
        total_requests: int
        success_rate: float
        models: list[ModelUsageStats]
        recent_activity: list[RecentActivity]

    # CLI-specific color utilities
    def get_success_color(rate: float) -> str:
        if rate >= 95:
            return "green"
        elif rate >= 80:
            return "yellow"
        return "red"

    def get_vendor_color(vendor: str, fallback: str = "white") -> str:
        colors = {
            "openai": "green",
            "anthropic": "bright_magenta",
            "google": "blue",
            "groq": "yellow",
            "mistral": "cyan",
            "cohere": "bright_red",
        }
        return colors.get(vendor.lower(), fallback)

    # Display functions
    def display_summary_panel(stats: UsageStatsResponse) -> None:
        success_color = get_success_color(stats.success_rate)
        summary_lines = [
            f"[bold cyan]Total Tokens:[/bold cyan]   {format_number(stats.total_tokens)}",
            f"[bold cyan]Total Cost:[/bold cyan]     {format_cost(stats.total_cost)}",
            f"[bold cyan]Total Requests:[/bold cyan] {format_number(stats.total_requests)}",
            f"[bold cyan]Success Rate:[/bold cyan]   "
            f"[{success_color}]{format_percentage(stats.success_rate)}[/{success_color}]",
        ]
        console.print(
            Panel(
                "\n".join(summary_lines),
                title="[bold magenta]AI Usage Summary[/bold magenta]",
                border_style="magenta",
                padding=(1, 2),
            )
        )

    def display_token_breakdown(stats: UsageStatsResponse) -> None:
        total = stats.total_tokens
        if total == 0:
            console.print("\n[dim]No token usage recorded.[/dim]")
            return
        input_pct = (stats.input_tokens / total) * 100
        output_pct = (stats.output_tokens / total) * 100
        console.print("\n[bold blue]Token Breakdown[/bold blue]")
        input_str = format_number(stats.input_tokens)
        output_str = format_number(stats.output_tokens)
        console.print(f"  [cyan]Input:[/cyan]  {input_str:>12} ({input_pct:.0f}%)")
        console.print(f"  [cyan]Output:[/cyan] {output_str:>12} ({output_pct:.0f}%)")
        bar_width = 40
        input_bars = int((input_pct / 100) * bar_width)
        output_bars = bar_width - input_bars
        bar = (
            f"[purple]{'█' * input_bars}[/purple]"
            f"[bright_magenta]{'█' * output_bars}[/bright_magenta]"
        )
        console.print(f"\n  {bar}")
        legend = "[purple]█ Input[/purple]  [bright_magenta]█ Output[/bright_magenta]"
        console.print(f"  {legend}")

    def display_model_usage(stats: UsageStatsResponse) -> None:
        if not stats.models:
            console.print("\n[dim]No model usage data available.[/dim]")
            return
        console.print("\n[bold blue]Model Usage[/bold blue]")
        table = Table(show_header=True, header_style="bold magenta", box=None)
        table.add_column("Model", style="cyan", no_wrap=True)
        table.add_column("Vendor", style="dim")
        table.add_column("Requests", justify="right")
        table.add_column("Tokens", justify="right")
        table.add_column("Cost", justify="right", style="green")
        table.add_column("Share", justify="right")
        for model in stats.models:
            vendor_color = get_vendor_color(model.vendor, model.vendor_color)
            table.add_row(
                model.model_title,
                f"[{vendor_color}]{model.vendor}[/{vendor_color}]",
                format_number(model.requests),
                format_number(model.tokens),
                format_cost(model.cost),
                format_percentage(model.percentage),
            )
        console.print(table)

    def display_recent_activity(stats: UsageStatsResponse) -> None:
        if not stats.recent_activity:
            console.print("\n[dim]No recent activity.[/dim]")
            return
        console.print("\n[bold blue]Recent Activity[/bold blue]")
        table = Table(show_header=True, header_style="bold magenta", box=None)
        table.add_column("Time", style="dim")
        table.add_column("Model", style="cyan")
        table.add_column("Action")
        table.add_column("Tokens", justify="right")
        table.add_column("Cost", justify="right", style="green")
        table.add_column("Status", justify="center")
        for activity in stats.recent_activity:
            try:
                dt = datetime.fromisoformat(activity.timestamp.replace("Z", "+00:00"))
                time_str = dt.strftime("%H:%M:%S")
            except ValueError:
                time_str = activity.timestamp[:8]
            status = "[green]OK[/green]" if activity.success else "[red]FAIL[/red]"
            table.add_row(
                time_str,
                activity.model,
                activity.action,
                format_number(activity.tokens),
                format_cost(activity.cost),
                status,
            )
        console.print(table)

    # Async fetch function
    async def get_usage_stats(
        base_url: str,
        filter_user_id: str | None = None,
        recent_limit: int = 10,
    ) -> UsageStatsResponse:
        api_url = f"{base_url}{APIEndpoints.AI_USAGE_STATS}"
        params: dict[str, str | int] = {"recent_limit": recent_limit}
        if filter_user_id:
            params["user_id"] = filter_user_id
        timeout = httpx.Timeout(Defaults.API_TIMEOUT)
        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                response = await client.get(api_url, params=params)
                response.raise_for_status()
                return UsageStatsResponse.model_validate(response.json())
            except httpx.ConnectError:
                raise ConnectionError(
                    f"Cannot connect to API server at {base_url}. "
                    "Make sure the application is running."
                ) from None
            except httpx.TimeoutException:
                raise TimeoutError(
                    f"API request timed out after {Defaults.API_TIMEOUT} seconds."
                ) from None
            except httpx.HTTPStatusError as e:
                raise RuntimeError(
                    f"API error {e.response.status_code}: {e.response.text}"
                ) from None

    # Main execution
    base_url = url or getattr(settings, "API_BASE_URL", "http://localhost:8000")
    try:
        stats = asyncio.run(get_usage_stats(base_url, user_id, recent))
        if json_output:
            print(json.dumps(stats.model_dump(), indent=2))
            return
        display_summary_panel(stats)
        display_token_breakdown(stats)
        display_model_usage(stats)
        display_recent_activity(stats)
    except ConnectionError as e:
        console.print(f"[red]Connection Error:[/red] {e}")
        sys.exit(1)
    except TimeoutError as e:
        console.print(f"[red]Timeout Error:[/red] {e}")
        sys.exit(1)
    except Exception as e:
        console.print(f"[red]Error:[/red] {e}")
        sys.exit(1)
{% endif %}


# ============================================================================
# Internal helper functions
# ============================================================================


async def _send_message(
    ai_service,
    message: str,
    conversation_id: str | None,
    user_id: str,
    stream: bool,
    verbose: bool,
) -> None:
    """Send a single message and display the response."""
    # Disable streaming for PUBLIC provider (fake streaming causes duplicates)
    use_streaming = stream
    if ai_service.config.provider == AIProvider.PUBLIC:
        use_streaming = False

    if use_streaming:
        await _stream_chat_response(
            ai_service, message, conversation_id, user_id, verbose=verbose
        )
    else:
        # Show thinking spinner for non-streaming responses
        from rich.live import Live
        from rich.spinner import Spinner

        spinner = Spinner("dots", text="Thinking...", style="bright_blue")
        spinner_live = Live(
            spinner, console=console, refresh_per_second=20, transient=True
        )
        spinner_live.start()

        try:
            response = await ai_service.chat(
                message=message,
                conversation_id=conversation_id,
                user_id=user_id,
            )
        finally:
            spinner_live.stop()

        # Use shared rendering functions
        from app.cli.ai_rendering import (
            render_ai_header,
            render_conversation_metadata,
            render_markdown_response,
        )

        # Show conversation info (only in verbose mode)
        conv_id = response.metadata.get("conversation_id", "unknown")
        conversation = ai_service.get_conversation(conv_id)
        if verbose and conversation:
            typer.echo(f"Conversation: {conversation.id}")
            if conversation.title:
                typer.echo(f"Title: {conversation.title}")
            console.print()

        # Render response
        render_ai_header(console, inline=True)
        render_markdown_response(console, response.content)

        # Show response metadata (only in verbose mode)
        if verbose and conversation:
            response_time = conversation.metadata.get("last_response_time_ms")
            render_conversation_metadata(
                console,
                conversation.id,
                message_count=conversation.get_message_count(),
                response_time=response_time,
            )


async def _interactive_chat_session(
    ai_service,
    conversation_id: str | None = None,
) -> None:
    """Start an interactive chat session with continuous conversation."""
    # Show welcome banner
    ai_config = get_ai_config(settings)
    welcome_text = (
        f"[bold cyan]AI Chat Session[/bold cyan]\n"
        f"[dim]Provider: {ai_config.provider} | Model: {ai_config.model}[/dim]\n"
        f"[dim]Type 'exit', 'quit', 'bye' or press Ctrl+C to end session[/dim]"
    )

    console.print(Panel(welcome_text, border_style="blue", expand=False))
    console.print()

    # Track conversation for context
    current_conversation_id = conversation_id

    while True:
        try:
            # Get user input with Rich prompt
            try:
                user_message = Prompt.ask(
                    "[bold green]You[/bold green]", console=console
                )
            except (KeyboardInterrupt, EOFError):
                console.print("\n[yellow]Chat session ended[/yellow]")
                break

            # Check for exit commands
            if user_message.lower().strip() in ["exit", "quit", "bye", "q"]:
                console.print("[yellow]Goodbye![/yellow]")
                break

            if not user_message.strip():
                console.print("[dim]Please enter a message or 'exit' to quit.[/dim]")
                continue

            # Disable streaming for PUBLIC provider
            use_streaming = True
            if ai_service.config.provider == AIProvider.PUBLIC:
                use_streaming = False

            try:
                if use_streaming:
                    returned_conversation_id = await _stream_chat_response(
                        ai_service,
                        user_message,
                        current_conversation_id,
                        "cli-user",
                    )
                    if returned_conversation_id:
                        current_conversation_id = returned_conversation_id
                else:
                    # Show thinking spinner for non-streaming responses
                    from rich.live import Live
                    from rich.spinner import Spinner

                    spinner = Spinner(
                        "dots", text="Thinking...", style="bright_blue"
                    )
                    spinner_live = Live(
                        spinner,
                        console=console,
                        refresh_per_second=20,
                        transient=True,
                    )
                    spinner_live.start()

                    try:
                        response = await ai_service.chat(
                            message=user_message,
                            conversation_id=current_conversation_id,
                            user_id="cli-user",
                        )
                    finally:
                        spinner_live.stop()

                    # Use shared rendering functions
                    from app.cli.ai_rendering import (
                        render_ai_header,
                        render_markdown_response,
                    )

                    render_ai_header(console, inline=True)
                    render_markdown_response(console, response.content)

                    # Update conversation reference
                    current_conversation_id = response.metadata.get(
                        "conversation_id", current_conversation_id
                    )
            except Exception as stream_error:
                console.print(f"[red]Error: {stream_error}[/red]")
                console.print(
                    "[dim]Try a different provider or check your connection.[/dim]"
                )

            console.print()  # Add space after response

        except Exception as e:
            console.print(f"[red]Error: {e}[/red]")
            console.print(
                "[dim]You can continue chatting or type 'exit' to quit.[/dim]"
            )


async def _stream_chat_response(
    ai_service,
    message: str,
    conversation_id: str | None,
    user_id: str,
    verbose: bool = False,
) -> str | None:
    """
    Stream chat response with real-time markdown rendering.

    Returns:
        The conversation ID for continuing the conversation, or None if interrupted.
    """
    import signal

    from rich.live import Live
    from rich.spinner import Spinner

    from app.cli.ai_rendering import StreamingMarkdownRenderer

    renderer = StreamingMarkdownRenderer(console)
    conversation_info = None
    response_time = None

    # Set up signal handler for graceful interruption
    interrupted = False

    def signal_handler(signum, frame):
        nonlocal interrupted
        interrupted = True

    old_handler = signal.signal(signal.SIGINT, signal_handler)

    try:
        header_shown = False
        import asyncio

        # Show thinking spinner initially
        spinner = Spinner("dots", text="Thinking...", style="bright_blue")
        spinner_live = Live(
            spinner, console=console, refresh_per_second=20, transient=True
        )
        spinner_live.start()

        try:
            processed_content = set()

            async with asyncio.timeout(30.0):  # 30 second timeout
                async for chunk in ai_service.stream_chat(
                    message=message,
                    conversation_id=conversation_id,
                    user_id=user_id,
                    stream_delta=True,
                ):
                    if interrupted:
                        spinner_live.stop()
                        console.print(
                            "\nStreaming interrupted", style="yellow"
                        )
                        break

                    # Skip duplicate content (handles fake streaming providers)
                    if chunk.content in processed_content:
                        if chunk.is_final:
                            conversation_info = chunk.conversation_id
                            response_time = chunk.metadata.get("response_time_ms")
                        continue

                    processed_content.add(chunk.content)

                    if chunk.is_delta and chunk.content:
                        if not header_shown:
                            spinner_live.stop()
                            console.print(": ", style="bright_blue", end="")
                            header_shown = True
                        renderer.add_delta(chunk.content)

                    if chunk.is_final:
                        conversation_info = chunk.conversation_id
                        response_time = chunk.metadata.get("response_time_ms")
                        break
        except TimeoutError:
            spinner_live.stop()
            console.print("\nError: Request timed out after 30 seconds", style="red")
            return None
        finally:
            if spinner_live.is_started:
                spinner_live.stop()

        if not interrupted:
            renderer.finalize()
            console.print("\n")

            if verbose and conversation_info:
                conversation = ai_service.get_conversation(conversation_info)
                if conversation:
                    console.print(f"Conversation: {conversation.id}", style="dim")
                    console.print(
                        f"Messages: {conversation.get_message_count()}", style="dim"
                    )
                    if response_time:
                        console.print(
                            f"Response time: {response_time:.1f}ms", style="dim"
                        )

    except Exception as e:
        if not interrupted:
            console.print(f"Streaming error: {e}", style="red")
        raise

    finally:
        signal.signal(signal.SIGINT, old_handler)

    return conversation_info if not interrupted else None


if __name__ == "__main__":
    app()
