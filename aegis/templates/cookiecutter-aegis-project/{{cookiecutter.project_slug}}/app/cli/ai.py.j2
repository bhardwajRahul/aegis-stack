"""
AI service CLI commands.

Command-line interface for AI service management and chat functionality.
"""

import logging
from contextlib import contextmanager

import typer
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from rich.table import Table

from ..core.config import settings
from ..services.ai.config import get_ai_config
from ..services.ai.models import (
    AIProvider,
    MessageRole,
    get_free_providers,
    get_provider_capabilities,
)

app = typer.Typer(help="AI service management and chat commands")
console = Console()


@contextmanager
def suppress_logs(level: int = logging.ERROR):
    """
    Context manager to temporarily suppress logs during interactive chat.

    Sets the root logger to ERROR level to hide INFO/DEBUG/WARNING logs while
    preserving ERROR logs for critical issues.

    Args:
        level: Minimum log level to show (default: ERROR)
    """
    # Get the root logger and remember original level
    root_logger = logging.getLogger()
    original_level = root_logger.level

    try:
        # Temporarily raise log level to suppress info/debug logs
        root_logger.setLevel(level)
        yield
    finally:
        # Restore original log level
        root_logger.setLevel(original_level)


@app.command()
def status() -> None:
    """Show AI service status, configuration, and validation."""
    ai_config = get_ai_config(settings)

    typer.secho("AI Service Status", fg=typer.colors.BLUE, bold=True)
    typer.secho("=" * 40, dim=True)

    # Basic info
    typer.echo(
        typer.style("Engine: ", fg=typer.colors.CYAN) + "{{ cookiecutter.ai_framework }}"
    )
    status_color = typer.colors.GREEN if ai_config.enabled else typer.colors.RED
    status_text = "Enabled" if ai_config.enabled else "Disabled"
    typer.echo(
        typer.style("Status: ", fg=typer.colors.CYAN)
        + typer.style(status_text, fg=status_color)
    )
    typer.echo(
        typer.style("Provider: ", fg=typer.colors.CYAN) + ai_config.provider.value
    )
    typer.echo(typer.style("Model: ", fg=typer.colors.CYAN) + str(ai_config.model))
    typer.echo(
        typer.style("Temperature: ", fg=typer.colors.CYAN)
        + str(ai_config.temperature)
    )
    typer.echo(
        typer.style("Max Tokens: ", fg=typer.colors.CYAN) + str(ai_config.max_tokens)
    )

    # API Key status
    provider_config = ai_config.get_provider_config(settings)
    api_key_color = typer.colors.GREEN if provider_config.api_key else typer.colors.RED
    api_key_text = "Set" if provider_config.api_key else "Not set"
    typer.echo(
        typer.style("API Key: ", fg=typer.colors.CYAN)
        + typer.style(api_key_text, fg=api_key_color)
    )

    # Validation
    typer.echo("")
    errors = ai_config.validate_configuration(settings)
    if not errors:
        typer.secho("✓ Configuration valid", fg=typer.colors.GREEN)
        capabilities = get_provider_capabilities(ai_config.provider)
        if capabilities.free_tier_available:
            typer.echo("  " + typer.style("Free tier", fg=typer.colors.CYAN))
        if capabilities.supports_streaming:
            typer.echo("  " + typer.style("Streaming supported", fg=typer.colors.CYAN))
    else:
        typer.secho("✗ Configuration issues:", fg=typer.colors.RED)
        for error in errors:
            typer.echo("  " + typer.style("•", fg=typer.colors.RED) + f" {error}")

        # Suggest free providers if API key issues
        if any("API key" in error for error in errors):
            free_providers = get_free_providers()
            if free_providers:
                providers_list = ", ".join(p.value for p in free_providers)
                typer.echo("")
                typer.echo(
                    typer.style("Tip: ", fg=typer.colors.YELLOW)
                    + f"Try free providers: {providers_list}"
                )

    # Available providers count
    available = ai_config.get_available_providers(settings)
    typer.echo("")
    typer.echo(
        typer.style("Available providers: ", fg=typer.colors.CYAN)
        + f"{len(available)} (run 'ai providers' to list)"
    )


@app.command()
def providers() -> None:
    """List all available AI providers."""
    ai_config = get_ai_config(settings)
    available = ai_config.get_available_providers(settings)
    free_providers = get_free_providers()

    table = Table(title="AI Providers", width=75)
    table.add_column("Provider", style="cyan", width=9)
    table.add_column("Status", style="green", width=26, no_wrap=True)
    table.add_column("Free", style="yellow", width=4)
    table.add_column("Features", style="blue", width=18)

    for provider in AIProvider:
        capabilities = get_provider_capabilities(provider)
        is_available = provider in available
        is_current = provider == ai_config.provider

        if is_available:
            status = "Available"
        else:
            # Make status more informative about what's missing
            if provider == AIProvider.PUBLIC:
                status = "[FAIL] Error"  # Shouldn't happen for PUBLIC
            else:
                # Show abbreviated environment variable name
                env_var = f"{provider.value.upper()}_API_KEY"
                status = f"[red]Need {env_var}[/red]"

        if is_current:
            status += " (current)"

        free_tier = "Yes" if provider in free_providers else "No"

        features = []
        if capabilities.supports_streaming:
            features.append("Stream")
        if capabilities.supports_function_calling:
            features.append("Functions")
        if capabilities.supports_vision:
            features.append("Vision")

        table.add_row(
            provider.value,
            status,
            free_tier,
            ", ".join(features) if features else "Basic",
        )

    console.print(table)


@app.command()
def chat(
    message: str | None = typer.Argument(None, help="Message to send to AI"),
    stream: bool = typer.Option(
        True, "--stream/--no-stream", help="Enable streaming output"
    ),
    conversation_id: str | None = typer.Option(
        None, "--conversation-id", "-c", help="Continue existing conversation"
    ),
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show conversation metadata"
    ),
) -> None:
    """Send a chat message or start interactive session.

    Examples:
        ai chat "What is Python?"     - Send a single message
        ai chat                       - Start interactive session
        ai chat -c abc123 "Continue"  - Continue a conversation
    """
    import asyncio

    from app.services.ai.service import AIService

    async def run_chat() -> None:
        try:
            with suppress_logs():
                ai_service = AIService(settings)

                if message:
                    # Single message mode
                    await _send_message(
                        ai_service, message, conversation_id, user_id, stream, verbose
                    )
                else:
                    # Interactive session mode
                    await _interactive_chat_session(ai_service, conversation_id)
        except KeyboardInterrupt:
            typer.echo("\nChat interrupted", err=True)
            raise typer.Exit(1)
        except Exception as e:
            typer.echo(f"Error: {e}", err=True)
            raise typer.Exit(1)

    asyncio.run(run_chat())


@app.command()
def conversations(
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
    limit: int = typer.Option(
        10, "--limit", "-l", help="Number of conversations to show"
    ),
) -> None:
    """List conversations for a user."""
    from app.services.ai.service import AIService

    with suppress_logs():
        ai_service = AIService(settings)
    convos = ai_service.list_conversations(user_id)[:limit]

    if not convos:
        typer.echo(f"No conversations found for user: {user_id}")
        return

    typer.echo(f"Conversations for {user_id}:")
    typer.echo("")

    for conv in convos:
        title = conv.title or "Untitled"
        messages = conv.get_message_count()
        updated = conv.updated_at.strftime("%Y-%m-%d %H:%M")

        typer.echo(f"• {conv.id[:8]}... - {title}")
        typer.echo(f"  {messages} messages | {updated}")
        typer.echo("")


@app.command()
def history(
    conversation_id: str = typer.Argument(..., help="Conversation ID"),
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
) -> None:
    """View conversation history."""
    from app.services.ai.service import AIService

    with suppress_logs():
        ai_service = AIService(settings)
    conversation = ai_service.get_conversation(conversation_id)

    if not conversation:
        typer.echo(f"Error: Conversation not found: {conversation_id}")
        raise typer.Exit(1)

    # Check if user owns conversation
    if conversation.metadata.get("user_id") != user_id:
        typer.echo("Error: Access denied: You don't own this conversation")
        raise typer.Exit(1)

    typer.echo(f"Conversation: {conversation_id}")
    if conversation.title:
        typer.echo(f"Title: {conversation.title}")
    typer.echo(f"Provider: {conversation.provider.value}")
    typer.echo(f"Messages: {conversation.get_message_count()}")
    typer.echo("")

    for i, msg in enumerate(conversation.messages):
        timestamp = msg.timestamp.strftime("%H:%M:%S")
        role_icon = "" if msg.role == MessageRole.USER else ""

        typer.echo(f"{role_icon} [{timestamp}] {msg.content}")
        if i < len(conversation.messages) - 1:
            typer.echo("")


# ============================================================================
# Internal helper functions
# ============================================================================


async def _send_message(
    ai_service,
    message: str,
    conversation_id: str | None,
    user_id: str,
    stream: bool,
    verbose: bool,
) -> None:
    """Send a single message and display the response."""
    # Disable streaming for PUBLIC provider (fake streaming causes duplicates)
    use_streaming = stream
    if ai_service.config.provider == AIProvider.PUBLIC:
        use_streaming = False

    if use_streaming:
        await _stream_chat_response(
            ai_service, message, conversation_id, user_id, verbose=verbose
        )
    else:
        # Show thinking spinner for non-streaming responses
        from rich.live import Live
        from rich.spinner import Spinner

        spinner = Spinner("dots", text="Thinking...", style="bright_blue")
        spinner_live = Live(
            spinner, console=console, refresh_per_second=20, transient=True
        )
        spinner_live.start()

        try:
            response = await ai_service.chat(
                message=message,
                conversation_id=conversation_id,
                user_id=user_id,
            )
        finally:
            spinner_live.stop()

        # Use shared rendering functions
        from app.cli.ai_rendering import (
            render_ai_header,
            render_conversation_metadata,
            render_markdown_response,
        )

        # Show conversation info (only in verbose mode)
        conv_id = response.metadata.get("conversation_id", "unknown")
        conversation = ai_service.get_conversation(conv_id)
        if verbose and conversation:
            typer.echo(f"Conversation: {conversation.id}")
            if conversation.title:
                typer.echo(f"Title: {conversation.title}")
            console.print()

        # Render response
        render_ai_header(console, inline=True)
        render_markdown_response(console, response.content)

        # Show response metadata (only in verbose mode)
        if verbose and conversation:
            response_time = conversation.metadata.get("last_response_time_ms")
            render_conversation_metadata(
                console,
                conversation.id,
                message_count=conversation.get_message_count(),
                response_time=response_time,
            )


async def _interactive_chat_session(
    ai_service,
    conversation_id: str | None = None,
) -> None:
    """Start an interactive chat session with continuous conversation."""
    # Show welcome banner
    ai_config = get_ai_config(settings)
    welcome_text = (
        f"[bold cyan]AI Chat Session[/bold cyan]\n"
        f"[dim]Provider: {ai_config.provider} | Model: {ai_config.model}[/dim]\n"
        f"[dim]Type 'exit', 'quit', 'bye' or press Ctrl+C to end session[/dim]"
    )

    console.print(Panel(welcome_text, border_style="blue", expand=False))
    console.print()

    # Track conversation for context
    current_conversation_id = conversation_id

    while True:
        try:
            # Get user input with Rich prompt
            try:
                user_message = Prompt.ask(
                    "[bold green]You[/bold green]", console=console
                )
            except (KeyboardInterrupt, EOFError):
                console.print("\n[yellow]Chat session ended[/yellow]")
                break

            # Check for exit commands
            if user_message.lower().strip() in ["exit", "quit", "bye", "q"]:
                console.print("[yellow]Goodbye![/yellow]")
                break

            if not user_message.strip():
                console.print("[dim]Please enter a message or 'exit' to quit.[/dim]")
                continue

            # Disable streaming for PUBLIC provider
            use_streaming = True
            if ai_service.config.provider == AIProvider.PUBLIC:
                use_streaming = False

            try:
                if use_streaming:
                    returned_conversation_id = await _stream_chat_response(
                        ai_service,
                        user_message,
                        current_conversation_id,
                        "cli-user",
                    )
                    if returned_conversation_id:
                        current_conversation_id = returned_conversation_id
                else:
                    # Show thinking spinner for non-streaming responses
                    from rich.live import Live
                    from rich.spinner import Spinner

                    spinner = Spinner(
                        "dots", text="Thinking...", style="bright_blue"
                    )
                    spinner_live = Live(
                        spinner,
                        console=console,
                        refresh_per_second=20,
                        transient=True,
                    )
                    spinner_live.start()

                    try:
                        response = await ai_service.chat(
                            message=user_message,
                            conversation_id=current_conversation_id,
                            user_id="cli-user",
                        )
                    finally:
                        spinner_live.stop()

                    # Use shared rendering functions
                    from app.cli.ai_rendering import (
                        render_ai_header,
                        render_markdown_response,
                    )

                    render_ai_header(console, inline=True)
                    render_markdown_response(console, response.content)

                    # Update conversation reference
                    current_conversation_id = response.metadata.get(
                        "conversation_id", current_conversation_id
                    )
            except Exception as stream_error:
                console.print(f"[red]Error: {stream_error}[/red]")
                console.print(
                    "[dim]Try a different provider or check your connection.[/dim]"
                )

            console.print()  # Add space after response

        except Exception as e:
            console.print(f"[red]Error: {e}[/red]")
            console.print(
                "[dim]You can continue chatting or type 'exit' to quit.[/dim]"
            )


async def _stream_chat_response(
    ai_service,
    message: str,
    conversation_id: str | None,
    user_id: str,
    verbose: bool = False,
) -> str | None:
    """
    Stream chat response with real-time markdown rendering.

    Returns:
        The conversation ID for continuing the conversation, or None if interrupted.
    """
    import signal

    from rich.live import Live
    from rich.spinner import Spinner

    from app.cli.ai_rendering import StreamingMarkdownRenderer

    renderer = StreamingMarkdownRenderer(console)
    conversation_info = None
    response_time = None

    # Set up signal handler for graceful interruption
    interrupted = False

    def signal_handler(signum, frame):
        nonlocal interrupted
        interrupted = True

    old_handler = signal.signal(signal.SIGINT, signal_handler)

    try:
        header_shown = False
        import asyncio

        # Show thinking spinner initially
        spinner = Spinner("dots", text="Thinking...", style="bright_blue")
        spinner_live = Live(
            spinner, console=console, refresh_per_second=20, transient=True
        )
        spinner_live.start()

        try:
            processed_content = set()

            async with asyncio.timeout(30.0):  # 30 second timeout
                async for chunk in ai_service.stream_chat(
                    message=message,
                    conversation_id=conversation_id,
                    user_id=user_id,
                    stream_delta=True,
                ):
                    if interrupted:
                        spinner_live.stop()
                        console.print(
                            "\nStreaming interrupted", style="yellow"
                        )
                        break

                    # Skip duplicate content (handles fake streaming providers)
                    if chunk.content in processed_content:
                        if chunk.is_final:
                            conversation_info = chunk.conversation_id
                            response_time = chunk.metadata.get("response_time_ms")
                        continue

                    processed_content.add(chunk.content)

                    if chunk.is_delta and chunk.content:
                        if not header_shown:
                            spinner_live.stop()
                            console.print(": ", style="bright_blue", end="")
                            header_shown = True
                        renderer.add_delta(chunk.content)

                    if chunk.is_final:
                        conversation_info = chunk.conversation_id
                        response_time = chunk.metadata.get("response_time_ms")
                        break
        except TimeoutError:
            spinner_live.stop()
            console.print("\nError: Request timed out after 30 seconds", style="red")
            return None
        finally:
            if spinner_live.is_started:
                spinner_live.stop()

        if not interrupted:
            renderer.finalize()
            console.print("\n")

            if verbose and conversation_info:
                conversation = ai_service.get_conversation(conversation_info)
                if conversation:
                    console.print(f"Conversation: {conversation.id}", style="dim")
                    console.print(
                        f"Messages: {conversation.get_message_count()}", style="dim"
                    )
                    if response_time:
                        console.print(
                            f"Response time: {response_time:.1f}ms", style="dim"
                        )

    except Exception as e:
        if not interrupted:
            console.print(f"Streaming error: {e}", style="red")
        raise

    finally:
        signal.signal(signal.SIGINT, old_handler)

    return conversation_info if not interrupted else None


if __name__ == "__main__":
    app()
