"""
AI service core implementation.

This module provides the main AIService class that handles AI chat functionality,
conversation management, and provider integration.
"""

import uuid
from collections.abc import AsyncIterator
from datetime import UTC, datetime
from typing import Any

{% if ai_framework == "pydantic-ai" %}
from pydantic_ai.exceptions import ModelRetry, UnexpectedModelBehavior
{% endif %}
{% if ai_backend != "memory" %}
from sqlmodel import select

from app.core.db import db_session
from .models.llm import LargeLanguageModel, LLMPrice, LLMUsage
{% endif %}

from app.core.log import logger

from .config import get_ai_config
from .conversation import ConversationManager
from .models import (
    Conversation,
    ConversationMessage,
    MessageRole,
    StreamingConversation,
    StreamingMessage,
)
{% if ai_framework == "pydantic-ai" %}
from .providers import get_agent
{% else %}
from .providers import get_llm, SYSTEM_PROMPT
{% endif %}


class AIServiceError(Exception):
    """Base exception for AI service errors."""

    pass


class ProviderError(AIServiceError):
    """Exception raised when AI provider fails."""

    pass


class ConversationError(AIServiceError):
    """Exception raised when conversation management fails."""

    pass


class AIService:
    """
    Core AI service for chat functionality.

    Handles chat completions, conversation management, and provider abstraction.
    """

    def __init__(self, settings: Any):
        """Initialize AI service with configuration."""
        self.settings = settings
        self.config = get_ai_config(settings)
        self.conversation_manager = ConversationManager()

    async def chat(
        self, message: str, conversation_id: str | None = None, user_id: str = "default"
    ) -> ConversationMessage:
        """
        Send a chat message and get AI response.

        Args:
            message: The user's message
            conversation_id: Optional conversation ID (creates new if None)
            user_id: User identifier for conversation ownership

        Returns:
            ConversationMessage: The AI's response message

        Raises:
            AIServiceError: If service is disabled or not configured
            ProviderError: If AI provider fails
            ConversationError: If conversation management fails
        """
        if not self.config.enabled:
            raise AIServiceError("AI service is disabled")

        try:
            # Setup conversation and add user message
            conversation = self._setup_conversation(message, conversation_id, user_id)

{% if ai_framework == "pydantic-ai" %}
            # Prepare agent and conversation context
            agent, conversation_context = self._prepare_agent_and_context(conversation)

            # Get AI response
            start_time = datetime.now(UTC)
            result = await agent.run(conversation_context)
            end_time = datetime.now(UTC)
            response_time_ms = (end_time - start_time).total_seconds() * 1000

            # Add AI response to conversation
            ai_message = conversation.add_message(
                MessageRole.ASSISTANT, result.output, message_id=str(uuid.uuid4())
            )
{% else %}
            # Prepare LLM and messages
            llm, messages = self._prepare_llm_and_messages(conversation)

            # Get AI response
            start_time = datetime.now(UTC)
            result = await llm.ainvoke(messages)
            end_time = datetime.now(UTC)
            response_time_ms = (end_time - start_time).total_seconds() * 1000

            # Add AI response to conversation
            ai_message = conversation.add_message(
                MessageRole.ASSISTANT, result.content, message_id=str(uuid.uuid4())
            )
{% endif %}

            # Store conversation ID in message metadata for easy lookup
            ai_message.metadata["conversation_id"] = conversation.id

{% if ai_backend != "memory" %}
            # Record usage tracking
            usage = self._extract_usage(result)
            self._record_usage("chat", usage, user_id)
{% endif %}

            # Finalize conversation (update metadata and save)
            self._finalize_conversation(conversation, response_time_ms)

            return ai_message

{% if ai_framework == "pydantic-ai" %}
        except (ModelRetry, UnexpectedModelBehavior) as e:
            error_msg = f"AI provider error: {e}"
            logger.error(error_msg)
            raise ProviderError(error_msg) from e
{% endif %}
        except Exception as e:
            error_msg = f"Chat processing failed: {e}"
            logger.error(error_msg)
            raise AIServiceError(error_msg) from e

    async def stream_chat(
        self,
        message: str,
        conversation_id: str | None = None,
        user_id: str = "default",
        stream_delta: bool = False,
    ) -> AsyncIterator[StreamingMessage]:
        """
        Stream a chat message with real-time response generation.

        Args:
            message: The user's message
            conversation_id: Optional conversation ID (creates new if None)
            user_id: User identifier for conversation ownership
            stream_delta: Whether to stream delta changes or full content

        Yields:
            StreamingMessage: Real-time message chunks

        Raises:
            AIServiceError: If service is disabled or not configured
            ProviderError: If AI provider fails
            ConversationError: If conversation management fails
        """
        if not self.config.enabled:
            raise AIServiceError("AI service is disabled")

        try:
            # Setup conversation and add user message
            conversation = self._setup_conversation(message, conversation_id, user_id)

            # Create streaming conversation wrapper
            streaming_conv = StreamingConversation(conversation=conversation)
            streaming_conv.reset_stream()

{% if ai_framework == "pydantic-ai" %}
            # Prepare agent and conversation context
            agent, conversation_context = self._prepare_agent_and_context(conversation)
{% else %}
            # Prepare LLM and messages
            llm, messages = self._prepare_llm_and_messages(conversation)
{% endif %}

            # Start streaming
            start_time = datetime.now(UTC)

            # Generate a message ID for the streaming response
            message_id = str(uuid.uuid4())

{% if ai_framework == "pydantic-ai" %}
            # Use PydanticAI's run_stream method for streaming
{% if ai_backend != "memory" %}
            stream_usage: dict[str, int] = {"input_tokens": 0, "output_tokens": 0}
{% endif %}
            async with agent.run_stream(conversation_context) as result:
                # Stream text chunks
                async for text_chunk in result.stream_text(delta=stream_delta):
                    # Accumulate content
                    total_content = streaming_conv.accumulate_content(
                        text_chunk, is_delta=stream_delta
                    )

                    # Yield streaming message chunk
                    yield StreamingMessage(
                        content=text_chunk if stream_delta else total_content,
                        is_final=False,
                        is_delta=stream_delta,
                        message_id=message_id,
                        conversation_id=conversation.id,
                        metadata={
                            "provider": self.config.provider,
                            "model": self.config.model,
                            "stream_delta": stream_delta,
                        },
                    )

{% if ai_backend != "memory" %}
                # Capture usage after streaming completes (still inside async with)
                if hasattr(result, "usage") and callable(result.usage):
                    usage_obj = result.usage()
                    if usage_obj:
                        stream_usage = {
                            "input_tokens": usage_obj.request_tokens or 0,
                            "output_tokens": usage_obj.response_tokens or 0,
                        }
{% endif %}
{% else %}
            # Use LangChain's astream method for streaming
            async for chunk in llm.astream(messages):
                # Get content from chunk
                text_chunk = chunk.content if hasattr(chunk, 'content') else str(chunk)
                if not text_chunk:
                    continue

                # Accumulate content (LangChain always streams deltas)
                total_content = streaming_conv.accumulate_content(
                    text_chunk, is_delta=True
                )

                # Yield streaming message chunk
                yield StreamingMessage(
                    content=text_chunk if stream_delta else total_content,
                    is_final=False,
                    is_delta=stream_delta,
                    message_id=message_id,
                    conversation_id=conversation.id,
                    metadata={
                        "provider": self.config.provider,
                        "model": self.config.model,
                        "stream_delta": stream_delta,
                    },
                )
{% endif %}

            end_time = datetime.now(UTC)
            response_time_ms = (end_time - start_time).total_seconds() * 1000

            # Add final message to conversation using accumulated streaming content
            final_content = streaming_conv.accumulated_content or "No content received"
            ai_message = conversation.add_message(
                MessageRole.ASSISTANT, final_content, message_id=message_id
            )

            # Store conversation metadata
            ai_message.metadata["conversation_id"] = conversation.id

{% if ai_backend != "memory" %}
{% if ai_framework == "pydantic-ai" %}
            # Record usage tracking (PydanticAI provides streaming usage)
            self._record_usage("stream_chat", stream_usage, user_id)
{% else %}
            # LangChain streaming doesn't provide token counts
            # Record with zero tokens - actual usage tracking requires non-streaming
            self._record_usage("stream_chat", {"input_tokens": 0, "output_tokens": 0}, user_id)
{% endif %}
{% endif %}

            # Finalize conversation (update metadata and save)
            self._finalize_conversation(
                conversation, response_time_ms, is_streaming=True
            )

            # Yield final streaming message
            yield StreamingMessage(
                content=final_content,
                is_final=True,
                is_delta=False,
                message_id=message_id,
                conversation_id=conversation.id,
                metadata={
                    "provider": self.config.provider,
                    "model": self.config.model,
                    "response_time_ms": response_time_ms,
                    "stream_complete": True,
                },
            )

{% if ai_framework == "pydantic-ai" %}
        except (ModelRetry, UnexpectedModelBehavior) as e:
            error_msg = f"AI provider streaming error: {e}"
            logger.error(error_msg)
            raise ProviderError(error_msg) from e
{% endif %}
        except Exception as e:
            error_msg = f"Streaming failed: {e}"
            logger.error(error_msg)
            raise AIServiceError(error_msg) from e

    def _setup_conversation(
        self,
        message: str,
        conversation_id: str | None,
        user_id: str,
    ) -> Conversation:
        """
        Get or create conversation and add user message.

        Args:
            message: The user's message
            conversation_id: Optional conversation ID (creates new if None)
            user_id: User identifier for conversation ownership

        Returns:
            Conversation: The conversation with user message added

        Raises:
            ConversationError: If conversation_id provided but not found
        """
        # Get or create conversation
        if conversation_id:
            conversation = self.conversation_manager.get_conversation(conversation_id)
            if not conversation:
                raise ConversationError(f"Conversation {conversation_id} not found")
        else:
            conversation = self.conversation_manager.create_conversation(
                provider=self.config.provider,
                model=self.config.model,
                user_id=user_id,
            )

        # Add user message to conversation
        conversation.add_message(MessageRole.USER, message)

        return conversation

{% if ai_framework == "pydantic-ai" %}
    def _prepare_agent_and_context(self, conversation: Conversation) -> tuple[Any, str]:
        """
        Create agent for request and build conversation context.

        Args:
            conversation: The conversation to prepare context from

        Returns:
            tuple[Any, str]: (agent instance, conversation context string)
        """
        # Create agent for this request
        agent = get_agent(self.config, self.settings)

        # Build conversation context for AI
        conversation_context = self._build_conversation_context(conversation)

        return agent, conversation_context
{% else %}
    def _prepare_llm_and_messages(self, conversation: Conversation) -> tuple[Any, list]:
        """
        Create LLM instance and build message list for LangChain.

        Args:
            conversation: The conversation to prepare messages from

        Returns:
            tuple[Any, list]: (LLM instance, list of message tuples)
        """
        # Create LLM for this request
        llm = get_llm(self.config, self.settings)

        # Build messages list for LangChain
        messages = self._build_langchain_messages(conversation)

        return llm, messages

    def _build_langchain_messages(self, conversation: Conversation) -> list:
        """
        Build LangChain message list from conversation history.

        Args:
            conversation: The conversation with message history

        Returns:
            list: List of message tuples for LangChain
        """
        # Start with system prompt
        messages = [("system", SYSTEM_PROMPT)]

        if not conversation.messages:
            return messages

        # For continuous conversation, include recent message history
        # Limit to last 10 messages to manage context window
        recent_messages = conversation.messages[-10:]

        # Format messages for LangChain
        for msg in recent_messages:
            if msg.role == MessageRole.USER:
                messages.append(("human", msg.content))
            elif msg.role == MessageRole.ASSISTANT:
                messages.append(("assistant", msg.content))

        return messages
{% endif %}

    def _finalize_conversation(
        self,
        conversation: Conversation,
        response_time_ms: float,
        is_streaming: bool = False,
    ) -> None:
        """
        Update conversation metadata and save.

        Args:
            conversation: The conversation to finalize
            response_time_ms: Response time in milliseconds
            is_streaming: Whether this was a streaming response
        """
        # Update conversation metadata
        metadata_update = {
            "last_response_time_ms": response_time_ms,
            "total_messages": conversation.get_message_count(),
            "last_activity": datetime.now(UTC).isoformat(),
        }

        if is_streaming:
            metadata_update["streaming"] = True

        conversation.metadata.update(metadata_update)

        # Save conversation
        self.conversation_manager.save_conversation(conversation)

{% if ai_framework == "pydantic-ai" %}
    def _build_conversation_context(self, conversation: Conversation) -> str:
        """
        Build conversation context for AI from message history.

        Args:
            conversation: The conversation with message history

        Returns:
            str: Formatted conversation context for AI
        """
        if not conversation.messages:
            return ""

        # For continuous conversation, include recent message history
        # Limit to last 10 messages to manage context window
        recent_messages = conversation.messages[-10:]

        # Format messages for context
        context_parts = []
        for msg in recent_messages[:-1]:  # Exclude the latest message (just added)
            if msg.role == MessageRole.USER:
                context_parts.append(f"User: {msg.content}")
            elif msg.role == MessageRole.ASSISTANT:
                context_parts.append(f"Assistant: {msg.content}")

        # Add the current user message
        latest_message = conversation.get_last_message()
        if latest_message and latest_message.role == MessageRole.USER:
            if context_parts:
                # Include conversation history + current message
                return "\n".join(context_parts) + f"\n\nUser: {latest_message.content}"
            else:
                # First message in conversation
                return latest_message.content

        return ""
{% endif %}

    def get_conversation(self, conversation_id: str) -> Conversation | None:
        """Get a conversation by ID."""
        return self.conversation_manager.get_conversation(conversation_id)

    def list_conversations(self, user_id: str = "default") -> list[Conversation]:
        """List all conversations for a user."""
        return self.conversation_manager.list_conversations(user_id)

    def get_service_status(self) -> dict[str, Any]:
        """Get current service status and metrics."""
        # Use get_stats() which works for both memory and SQLite backends
        stats = self.conversation_manager.get_stats()
        total_conversations = stats["total_conversations"]

        return {
            "enabled": self.config.enabled,
            "provider": self.config.provider,
            "model": self.config.model,
            "agent_initialized": True,  # Agents created per request, always available
            "total_conversations": total_conversations,
            "configuration_valid": len(
                self.config.validate_configuration(self.settings)
            )
            == 0,
        }

    def validate_service(self) -> list[str]:
        """Validate service configuration and return any issues."""
        errors = []

        # Check configuration
        config_errors = self.config.validate_configuration(self.settings)
        errors.extend(config_errors)

        return errors

{% if ai_backend != "memory" %}
    def _extract_usage(self, result: Any) -> dict[str, int]:
        """
        Extract token usage from AI response.

        Args:
            result: The response from the AI provider

        Returns:
            dict: Token usage with input_tokens and output_tokens
        """
{% if ai_framework == "pydantic-ai" %}
        # PydanticAI stores usage in result.usage (attribute, not method)
        if hasattr(result, "usage") and result.usage and not callable(result.usage):
            return {
                "input_tokens": result.usage.request_tokens or 0,
                "output_tokens": result.usage.response_tokens or 0,
            }
{% else %}
        # LangChain stores usage in response_metadata
        if hasattr(result, "response_metadata"):
            token_usage = result.response_metadata.get("token_usage", {})
            return {
                "input_tokens": token_usage.get("prompt_tokens", 0),
                "output_tokens": token_usage.get("completion_tokens", 0),
            }
{% endif %}
        return {"input_tokens": 0, "output_tokens": 0}

    def _record_usage(
        self,
        action: str,
        usage: dict[str, int],
        user_id: str,
        success: bool = True,
        error_message: str | None = None,
    ) -> None:
        """
        Record LLM usage with cost calculation.

        Args:
            action: The action type (e.g., "chat", "stream_chat")
            usage: Token usage dict with input_tokens and output_tokens
            user_id: User identifier
            success: Whether the request succeeded
            error_message: Error message if request failed
        """
        model_name = self.config.model

        # Strip vendor prefix if present (e.g., "openai/gpt-4o" -> "gpt-4o")
        if "/" in model_name:
            model_name = model_name.split("/", 1)[1]

        try:
            with db_session() as session:
                # Look up the LLM by model_id
                stmt = select(LargeLanguageModel).where(
                    LargeLanguageModel.model_id == model_name
                )
                llm = session.exec(stmt).first()

                if not llm:
                    logger.warning("LLM not found for usage tracking", model_id=model_name)
                    return

                # Get latest price for this LLM
                price_stmt = (
                    select(LLMPrice)
                    .where(LLMPrice.llm_id == llm.id)
                    .order_by(LLMPrice.effective_date.desc())
                )
                price = session.exec(price_stmt).first()

                # Calculate cost
                if price:
                    input_cost = usage.get("input_tokens", 0) * price.input_cost_per_token
                    output_cost = usage.get("output_tokens", 0) * price.output_cost_per_token
                    total_cost = input_cost + output_cost
                else:
                    logger.warning("No price found for LLM", llm_id=llm.id)
                    total_cost = 0.0

                # Create usage record
                llm_usage = LLMUsage(
                    action=action,
                    llm_id=llm.id,
                    user_id=user_id,
                    timestamp=datetime.now(UTC),
                    input_tokens=usage.get("input_tokens", 0),
                    output_tokens=usage.get("output_tokens", 0),
                    total_cost=total_cost,
                    success=success,
                    error_message=error_message,
                )
                session.add(llm_usage)

                logger.debug(
                    "Recorded LLM usage",
                    model=model_name,
                    tokens=usage,
                    cost=total_cost,
                )

        except Exception as e:
            # Don't fail the request if usage tracking fails
            logger.error("Failed to record LLM usage", error=str(e))
{% endif %}
