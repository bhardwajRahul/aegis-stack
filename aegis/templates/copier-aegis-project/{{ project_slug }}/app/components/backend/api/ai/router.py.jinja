"""
AI service API router.

FastAPI router for AI chat endpoints implementing core chat functionality,
conversation management, and service status.
"""

import json
from collections.abc import AsyncIterator
{% if ai_backend != "memory" %}
from datetime import datetime
{% endif %}
from typing import Any

from app.core.config import settings
from app.core.log import logger
from app.services.ai.service import (
    AIService,
    AIServiceError,
    ConversationError,
    ProviderError,
)
from app.services.ai.voice import (
    AudioFormat,
    AudioInput,
    SpeechRequest,
    TranscriptionResult,
    VoiceChatResponse,
)
from fastapi import APIRouter, File, Form, HTTPException, Query, UploadFile
from fastapi.responses import Response, StreamingResponse
from pydantic import BaseModel, Field

router = APIRouter(prefix="/ai", tags=["ai"])

# Initialize AI service
ai_service = AIService(settings)


# Request/Response models
class ChatRequest(BaseModel):
    """Request model for chat messages."""

    message: str
    conversation_id: str | None = None
    user_id: str = "api-user"


class ChatResponse(BaseModel):
    """Response model for chat messages."""

    message_id: str
    content: str
    conversation_id: str
    response_time_ms: float | None = None


class ConversationSummary(BaseModel):
    """Summary model for conversation listing."""

    id: str
    title: str | None
    message_count: int
    last_activity: str
    provider: str
    model: str


{% if ai_backend != "memory" %}
class ModelUsageStats(BaseModel):
    """Usage statistics for a single model."""

    model_id: str
    model_title: str
    vendor: str
    vendor_color: str
    requests: int
    tokens: int
    cost: float
    percentage: float


class RecentActivity(BaseModel):
    """A single recent usage activity entry."""

    timestamp: str
    model: str
    input_tokens: int
    output_tokens: int
    cost: float
    success: bool
    action: str


class UsageStatsResponse(BaseModel):
    """Aggregated LLM usage statistics response."""

    total_tokens: int
    input_tokens: int
    output_tokens: int
    total_cost: float
    total_requests: int
    success_rate: float
    models: list[ModelUsageStats]
    recent_activity: list[RecentActivity]
{% endif %}


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    Send a chat message and get AI response.

    Args:
        request: Chat request with message and optional conversation ID

    Returns:
        ChatResponse: AI response with conversation details

    Raises:
        HTTPException: If chat processing fails
    """
    try:
        response_message = await ai_service.chat(
            message=request.message,
            conversation_id=request.conversation_id,
            user_id=request.user_id,
        )

        # Get updated conversation for metadata
        conversation_id = response_message.metadata.get("conversation_id")
        conversation = (
            ai_service.get_conversation(conversation_id) if conversation_id else None
        )
        response_time = None
        if conversation and "last_response_time_ms" in conversation.metadata:
            response_time = conversation.metadata["last_response_time_ms"]

        return ChatResponse(
            message_id=response_message.id,
            content=response_message.content,
            conversation_id=conversation.id if conversation else "unknown",
            response_time_ms=response_time,
        )

    except AIServiceError as e:
        raise HTTPException(status_code=503, detail=f"AI service error: {e}")
    except ProviderError as e:
        raise HTTPException(status_code=502, detail=f"AI provider error: {e}")
    except ConversationError as e:
        raise HTTPException(status_code=400, detail=f"Conversation error: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected error: {e}")


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest) -> StreamingResponse:
    """
    Stream a chat message with real-time Server-Sent Events.

    Args:
        request: Chat request with message and optional conversation ID

    Returns:
        StreamingResponse: SSE stream with real-time AI response

    Raises:
        HTTPException: If streaming fails
    """

    async def generate_sse_stream() -> AsyncIterator[str]:
        """Generate Server-Sent Events stream for chat response."""
        try:
            # Send initial connection event
            connect_data = {"status": "connected", "message": "Streaming started"}
            yield f"event: connect\ndata: {json.dumps(connect_data)}\n\n"

            # Stream the AI response
            async for chunk in ai_service.stream_chat(
                message=request.message,
                conversation_id=request.conversation_id,
                user_id=request.user_id,
                stream_delta=True,
            ):
                # Format chunk as SSE event
                event_data = {
                    "content": chunk.content,
                    "is_final": chunk.is_final,
                    "is_delta": chunk.is_delta,
                    "message_id": chunk.message_id,
                    "conversation_id": chunk.conversation_id,
                    "timestamp": chunk.timestamp.isoformat(),
                }

                # Add metadata for final chunk
                if chunk.is_final:
                    event_data.update(chunk.metadata)

                # Send chunk as SSE event
                event_type = "final" if chunk.is_final else "chunk"
                yield f"event: {event_type}\ndata: {json.dumps(event_data)}\n\n"

                # Break after final chunk
                if chunk.is_final:
                    break

            # Send stream complete event
            complete_data = {"status": "completed", "message": "Stream finished"}
            yield f"event: complete\ndata: {json.dumps(complete_data)}\n\n"

        except AIServiceError as e:
            error_data = {"error": "AI service error", "detail": str(e)}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
        except ProviderError as e:
            error_data = {"error": "AI provider error", "detail": str(e)}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
        except ConversationError as e:
            error_data = {"error": "Conversation error", "detail": str(e)}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
        except Exception as e:
            error_data = {"error": "Unexpected error", "detail": str(e)}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"

    # Create streaming response with proper SSE headers
    return StreamingResponse(
        generate_sse_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Content-Type",
        },
    )


@router.get("/conversations", response_model=list[ConversationSummary])
async def list_conversations(
    user_id: str = "api-user", limit: int = 50
) -> list[ConversationSummary]:
    """
    List conversations for a user.

    Args:
        user_id: User identifier
        limit: Maximum number of conversations to return

    Returns:
        List of conversation summaries
    """
    try:
        conversations = ai_service.list_conversations(user_id)[:limit]

        return [
            ConversationSummary(
                id=conv.id,
                title=conv.title,
                message_count=conv.get_message_count(),
                last_activity=conv.updated_at.isoformat(),
                provider=conv.provider.value,
                model=conv.model,
            )
            for conv in conversations
        ]

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to list conversations: {e}"
        )


@router.get("/conversations/{conversation_id}")
async def get_conversation(
    conversation_id: str, user_id: str = "api-user"
) -> dict[str, Any]:
    """
    Get a specific conversation with full message history.

    Args:
        conversation_id: The conversation identifier
        user_id: User identifier for access control

    Returns:
        Full conversation details with messages

    Raises:
        HTTPException: If conversation not found or access denied
    """
    try:
        conversation = ai_service.get_conversation(conversation_id)

        if not conversation:
            raise HTTPException(status_code=404, detail="Conversation not found")

        # Check access (basic user matching)
        if conversation.metadata.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="Access denied")

        return {
            "id": conversation.id,
            "title": conversation.title,
            "provider": conversation.provider.value,
            "model": conversation.model,
            "created_at": conversation.created_at.isoformat(),
            "updated_at": conversation.updated_at.isoformat(),
            "message_count": conversation.get_message_count(),
            "messages": [
                {
                    "id": msg.id,
                    "role": msg.role.value,
                    "content": msg.content,
                    "timestamp": msg.timestamp.isoformat(),
                }
                for msg in conversation.messages
            ],
            "metadata": conversation.metadata,
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get conversation: {e}")


@router.get("/health")
async def ai_health() -> dict[str, Any]:
    """
    AI service health endpoint.

    Returns comprehensive health status including configuration,
    conversation count, and service availability.
    """
    try:
        status = ai_service.get_service_status()
        validation_errors = ai_service.validate_service()

        return {
            "service": "ai",
            "status": "healthy" if not validation_errors else "unhealthy",
            "enabled": status["enabled"],
            "provider": status["provider"],
            "model": status["model"],
            "agent_ready": status["agent_initialized"],
            "total_conversations": status["total_conversations"],
            "configuration_valid": status["configuration_valid"],
            "validation_errors": validation_errors,
        }

    except Exception as e:
        return {
            "service": "ai",
            "status": "error",
            "error": str(e),
        }


@router.get("/version")
async def ai_version() -> dict[str, Any]:
    """AI service version and feature information."""
    return {
        "service": "ai",
        "engine": "{{ ai_framework }}",
        "version": "1.0",
        "features": [
            "chat",
            "conversation_management",
            "multi_provider_support",
            "health_monitoring",
            "api_endpoints",
            "cli_commands",
        ],
        "providers_supported": [
            "openai",
            "anthropic",
            "google",
            "groq",
            "mistral",
            "cohere",
        ],
    }


{% if ai_backend != "memory" %}
@router.get("/usage/stats", response_model=UsageStatsResponse)
async def get_usage_stats(
    user_id: str | None = None,
    start_time: datetime | None = None,
    end_time: datetime | None = None,
    recent_limit: int = 10,
) -> UsageStatsResponse:
    """
    Get aggregated LLM usage statistics.

    All aggregations are performed at the SQL level for scalability.
    Supports filtering by user and time range.

    Args:
        user_id: Optional filter by user
        start_time: Optional start of time range (ISO format)
        end_time: Optional end of time range (ISO format)
        recent_limit: Number of recent activities to return (default: 10)

    Returns:
        Usage statistics including totals, model breakdown, and recent activity
    """
    try:
        stats = ai_service.get_usage_stats(
            user_id=user_id,
            start_time=start_time,
            end_time=end_time,
            recent_limit=recent_limit,
        )
        return UsageStatsResponse(**stats)

    except Exception as e:
        logger.exception("Failed to get usage stats")
        raise HTTPException(
            status_code=500, detail="Failed to get usage stats"
        ) from e
{% endif %}


# Voice/STT Request/Response models
class TranscriptionSegmentResponse(BaseModel):
    """A segment of transcribed audio with timing."""

    text: str
    start: float
    end: float
    confidence: float | None = None


class TranscriptionResponse(BaseModel):
    """Response model for transcription results."""

    text: str
    language: str | None = None
    duration_seconds: float | None = None
    confidence: float | None = None
    provider: str
    segments: list[TranscriptionSegmentResponse] | None = None


class VoiceChatApiResponse(BaseModel):
    """Response model for voice chat endpoint."""

    transcription: TranscriptionResponse
    full_response: str
    voice_response: str
    conversation_id: str | None = None
    has_audio: bool = False  # Indicates if audio_response is available


@router.post("/voice-chat")
async def voice_chat(
    audio: UploadFile = File(..., description="Audio file to transcribe"),
    conversation_id: str | None = Query(None, description="Continue existing conversation"),
    voice_mode: bool = Query(False, description="Summarize response for spoken output"),
    return_audio: bool = Query(False, description="Return TTS audio of response"),
    user_id: str = Query("api-user", description="User identifier"),
) -> VoiceChatApiResponse | Response:
    """
    Process voice input: transcribe → chat → return response.

    This endpoint implements the full voice pipeline:
    1. **STT** - Transcribe uploaded audio to text
    2. **Agent** - Pass transcription to AI chat for processing
    3. **Response** - Return both full and voice-optimized responses
    4. **TTS** - Optionally synthesize response to audio (if return_audio=True)

    Supported audio formats: wav, mp3, m4a, webm, ogg, flac, mp4

    Args:
        audio: Audio file upload (multipart/form-data)
        conversation_id: Optional conversation ID to continue
        voice_mode: When True, summarize response for spoken output
        return_audio: When True, returns audio response instead of JSON
        user_id: User identifier for conversation ownership

    Returns:
        If return_audio=False: VoiceChatApiResponse with transcription and text
        If return_audio=True: Audio file (MP3) of the response

    Raises:
        HTTPException: 400 if audio format not supported
        HTTPException: 503 if AI/STT service error
    """
    try:
        # Determine audio format from filename
        if audio.filename:
            ext = audio.filename.rsplit(".", 1)[-1].lower()
        else:
            ext = "wav"  # Default

        try:
            audio_format = AudioFormat(ext)
        except ValueError:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported audio format: {ext}. "
                f"Supported: {', '.join(f.value for f in AudioFormat)}",
            )

        # Read audio content
        audio_content = await audio.read()

        if len(audio_content) == 0:
            raise HTTPException(status_code=400, detail="Empty audio file")

        # Create AudioInput
        audio_input = AudioInput(
            content=audio_content,
            format=audio_format,
        )

        # Process voice chat
        result = await ai_service.voice_chat(
            audio=audio_input,
            conversation_id=conversation_id,
            user_id=user_id,
            voice_mode=voice_mode,
            return_audio=return_audio,
        )

        # If audio response requested, return audio file directly
        if return_audio and result.audio_response:
            return Response(
                content=result.audio_response,
                media_type="audio/mpeg",
                headers={
                    "Content-Disposition": 'attachment; filename="response.mp3"',
                    "X-Transcription": result.transcription.text[:100],  # First 100 chars
                    "X-Conversation-Id": result.conversation_id or "",
                },
            )


        # Convert to API response
        transcription_segments = None
        if result.transcription.segments:
            transcription_segments = [
                TranscriptionSegmentResponse(
                    text=seg.text,
                    start=seg.start,
                    end=seg.end,
                    confidence=seg.confidence,
                )
                for seg in result.transcription.segments
            ]

        return VoiceChatApiResponse(
            transcription=TranscriptionResponse(
                text=result.transcription.text,
                language=result.transcription.language,
                duration_seconds=result.transcription.duration_seconds,
                confidence=result.transcription.confidence,
                provider=result.transcription.provider.value,
                segments=transcription_segments,
            ),
            full_response=result.full_response,
            voice_response=result.voice_response,
            conversation_id=result.conversation_id,
            has_audio=result.audio_response is not None,
        )

    except HTTPException:
        raise
    except AIServiceError as e:
        raise HTTPException(status_code=503, detail=f"AI service error: {e}")
    except ProviderError as e:
        raise HTTPException(status_code=502, detail=f"AI provider error: {e}")
    except Exception as e:
        logger.exception("Voice chat failed")
        raise HTTPException(status_code=500, detail=f"Voice chat failed: {e}")


@router.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(
    audio: UploadFile = File(..., description="Audio file to transcribe"),
    language: str | None = Query(None, description="ISO 639-1 language code (e.g., 'en')"),
) -> TranscriptionResponse:
    """
    Transcribe audio to text without chat processing.

    Use this endpoint when you only need speech-to-text transcription
    without AI chat response.

    Supported audio formats: wav, mp3, m4a, webm, ogg, flac, mp4

    Args:
        audio: Audio file upload (multipart/form-data)
        language: Optional language hint (auto-detected if not provided)

    Returns:
        TranscriptionResponse with transcribed text and metadata

    Raises:
        HTTPException: 400 if audio format not supported
        HTTPException: 503 if STT service error
    """
    try:
        # Determine audio format from filename
        if audio.filename:
            ext = audio.filename.rsplit(".", 1)[-1].lower()
        else:
            ext = "wav"

        try:
            audio_format = AudioFormat(ext)
        except ValueError:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported audio format: {ext}. "
                f"Supported: {', '.join(f.value for f in AudioFormat)}",
            )

        # Read audio content
        audio_content = await audio.read()

        if len(audio_content) == 0:
            raise HTTPException(status_code=400, detail="Empty audio file")

        # Create AudioInput
        audio_input = AudioInput(
            content=audio_content,
            format=audio_format,
            language=language,
        )

        # Transcribe
        result = await ai_service.stt.transcribe(audio_input)

        # Convert to API response
        segments = None
        if result.segments:
            segments = [
                TranscriptionSegmentResponse(
                    text=seg.text,
                    start=seg.start,
                    end=seg.end,
                    confidence=seg.confidence,
                )
                for seg in result.segments
            ]

        return TranscriptionResponse(
            text=result.text,
            language=result.language,
            duration_seconds=result.duration_seconds,
            confidence=result.confidence,
            provider=result.provider.value,
            segments=segments,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Transcription failed")
        raise HTTPException(status_code=503, detail=f"Transcription failed: {e}")


@router.get("/stt/status")
async def stt_status() -> dict[str, Any]:
    """
    Get STT (Speech-to-Text) service status.

    Returns current STT configuration and availability.
    """
    try:
        status = ai_service.stt.get_status()
        return {
            "service": "stt",
            "status": "available",
            **status,
        }
    except Exception as e:
        return {
            "service": "stt",
            "status": "error",
            "error": str(e),
        }


# ============================================================================
# TTS (Text-to-Speech) Endpoints
# ============================================================================


@router.post("/synthesize")
async def synthesize_speech(
    text: str = Form(..., description="Text to synthesize into speech"),
    voice: str | None = Form(None, description="Voice to use (provider-specific)"),
    speed: float = Form(1.0, ge=0.25, le=4.0, description="Speech speed (0.25-4.0)"),
) -> Response:
    """
    Synthesize speech from text.

    Converts text to speech audio using the configured TTS provider.

    Args:
        text: Text to synthesize
        voice: Optional voice ID (uses provider default if not specified)
        speed: Speech speed multiplier (0.25 to 4.0, default 1.0)

    Returns:
        Audio file response (MP3 format)

    Raises:
        HTTPException: 503 if TTS service error
    """
    try:
        request = SpeechRequest(text=text, voice=voice, speed=speed)
        result = await ai_service.tts.synthesize(request)

        return Response(
            content=result.audio,
            media_type="audio/mpeg",
            headers={
                "Content-Disposition": 'attachment; filename="speech.mp3"',
            },
        )

    except Exception as e:
        logger.exception("Speech synthesis failed")
        raise HTTPException(status_code=503, detail=f"Speech synthesis failed: {e}")


@router.post("/synthesize/stream")
async def synthesize_speech_stream(
    text: str = Form(..., description="Text to synthesize into speech"),
    voice: str | None = Form(None, description="Voice to use (provider-specific)"),
    speed: float = Form(1.0, ge=0.25, le=4.0, description="Speech speed (0.25-4.0)"),
) -> StreamingResponse:
    """
    Stream synthesized speech.

    Streams audio chunks as they are generated for lower latency.

    Args:
        text: Text to synthesize
        voice: Optional voice ID (uses provider default if not specified)
        speed: Speech speed multiplier (0.25 to 4.0, default 1.0)

    Returns:
        Streaming audio response (MP3 format)

    Raises:
        HTTPException: 503 if TTS service error
    """
    request = SpeechRequest(text=text, voice=voice, speed=speed)

    async def generate():
        try:
            async for chunk in ai_service.tts.synthesize_stream(request):
                yield chunk
        except Exception as e:
            logger.exception("Speech synthesis streaming failed")
            raise HTTPException(
                status_code=503, detail=f"Speech synthesis failed: {e}"
            )

    return StreamingResponse(
        generate(),
        media_type="audio/mpeg",
        headers={
            "Content-Disposition": 'attachment; filename="speech.mp3"',
        },
    )


@router.get("/tts/status")
async def tts_status() -> dict[str, Any]:
    """
    Get TTS (Text-to-Speech) service status.

    Returns current TTS configuration and availability.
    """
    try:
        status = ai_service.tts.get_status()
        return {
            "service": "tts",
            "status": "available",
            **status,
        }
    except Exception as e:
        return {
            "service": "tts",
            "status": "error",
            "error": str(e),
        }
